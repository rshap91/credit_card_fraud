{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rick.shapiro/anaconda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "from sklearn.covariance import EmpiricalCovariance, EllipticEnvelope, MinCovDet\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.mixture import BayesianGaussianMixture as BayesGMM\n",
    "\n",
    "\n",
    "from scipy.stats import entropy\n",
    "from scipy.cluster.hierarchy import dendrogram, fcluster, cophenet, set_link_color_palette\n",
    "from scipy.spatial.distance import squareform, mahalanobis, euclidean\n",
    "from fastcluster import linkage, pdist\n",
    "\n",
    "# saving models\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# incase we want to try some cleaning steps to see if it improves the model\n",
    "import Clean_Function_Helpers as cfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (9,6)\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "SEED = 1111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sample(df, mix, seed = SEED):\n",
    "    \"\"\"\n",
    "    Helper Function to get samples with different proportions of normal vs outliers.\n",
    "    \"\"\"\n",
    "    if isinstance(mix, int):\n",
    "        samp = df.sample(mix, random_state=SEED)\n",
    "        return samp\n",
    "    elif isinstance(mix, (tuple,list,np.ndarray)):\n",
    "        n_normal, n_outlier = mix\n",
    "        if n_normal is None: # use all \n",
    "            normal_samp = df[df.Class==0]\n",
    "        else:\n",
    "            normal_samp = df[df.Class==0].sample(n_normal, random_state=SEED)\n",
    "        if n_outlier is None:\n",
    "            outliers = df[df.Class==1]\n",
    "        else:\n",
    "            outliers = df[df.Class==1].sample(n_outlier, random_state=SEED)\n",
    "        samp = pd.concat([normal_samp, outliers])\n",
    "        return samp\n",
    "    else:\n",
    "        raise Exception('Invalid mix argument passed')\n",
    "\n",
    "\n",
    "def outliers_grid(est, train, test, default_params = {}, predict_function=None, **params):\n",
    "    param_keys = list(params.keys())\n",
    "    \n",
    "    initial_dic = dict.fromkeys(param_keys + ['f1','recall','precision'], 0)\n",
    "    best_f1 = initial_dic.copy()\n",
    "    best_recall = initial_dic.copy()\n",
    "    best_precision = initial_dic.copy()\n",
    "    \n",
    "    xtrain = train[sub_cols]\n",
    "    ytrain = train.Class\n",
    "\n",
    "    xtest = test[sub_cols]\n",
    "    ytest = test.Class\n",
    "    \n",
    "    all_params = product(*params.values())\n",
    "    print(' | '.join(param_keys))\n",
    "    print('-----------------------------')\n",
    "    for p in all_params:\n",
    "        p = dict(zip(param_keys, p))\n",
    "        print(p)\n",
    "        mod = est(**default_params, **p)\n",
    "        mod.fit(xtrain, ytrain)\n",
    "        if predict_function:\n",
    "            ypred = predict_function(mod, xtest)\n",
    "        else:\n",
    "            pred = mod.predict(xtest)\n",
    "            ypred = np.where(pred< 0, 1, 0)\n",
    "        f1 = metrics.f1_score(ytest,ypred)\n",
    "        recall = metrics.recall_score(ytest,ypred)\n",
    "        precision = metrics.precision_score(ytest,ypred)\n",
    "        if f1 > best_f1['f1']:\n",
    "            best_f1 = {**p, 'f1':f1, 'recall':recall, 'precision':precision}\n",
    "        if recall > best_recall['recall']:\n",
    "            best_recall = {**p, 'f1':f1, 'recall':recall, 'precision':precision}\n",
    "        if precision > best_precision['precision']:\n",
    "            best_precision = {**p, 'f1':f1, 'recall':recall, 'precision':precision}\n",
    "    print('=========== DONE ==========')\n",
    "    return best_f1, best_recall, best_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Taking two different approaches. \n",
    "\n",
    "    1. Try to model the difference between real and fraudulent charges.\n",
    "        - Classifiers like Logistic Regression, NaiveBayes, Tree Ensembles etc\n",
    "        - Sampling approaches over vs undersampling\n",
    "    2. Try to identify core boundary of real charges and identify anything outside this boundary as fraudulent.\n",
    "        - Covariance estimates, Local Outlier Factor, Clustering, One Class SVM, K-means, \n",
    "        Model-based bayesian clustering.\n",
    "        \n",
    "This notebook focuses on the second approach: using robust statistical methods as well as un-supervised learning to identify outliers. I'm also going to use the output of these models as inputs in ensemble models in the next notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outlier_ftr_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.998273\n",
       "1    0.001727\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('creditcard.csv')\n",
    "df.Class.value_counts()/df.Class.value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will test different transforms of the data\n",
    "\n",
    "sub_cols = df.columns.drop(['Time', 'Class'])\n",
    "\n",
    "scaled_df = cfh.scale_data(df, MinMaxScaler(), sub_cols)\n",
    "deskewed = cfh.deskew_df(scaled_df, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = df[sub_cols]\n",
    "y = df.Class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Covariance Determinant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCD is a method to compute a robust estimate for mean and covariance of a multivariate gaussian distributed dataset.\n",
    "Empirically computing mean and covariance is known to be very sensitive to outliers, so MCD finds a _core subset_ of the data that best represents the underlying distribution. From these robust estimates, we can determine outliers by a points (usually Mahalanobis) distance to the robust mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_NOTE Sklearn spits continually spits out RuntimeWarnings on the regular dataset leading me to believe that the data is not well approximated by a normal distribution. Running the Elliptic Envelope on scaled-deskewed data seems to solve the issue._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contamination | support_fraction\n",
      "-----------------------------\n",
      "{'contamination': 0.001, 'support_fraction': 0.6}\n",
      "{'contamination': 0.001, 'support_fraction': 0.7}\n",
      "{'contamination': 0.001, 'support_fraction': 0.8}\n",
      "{'contamination': 0.001, 'support_fraction': 0.9}\n",
      "{'contamination': 0.005, 'support_fraction': 0.6}\n",
      "{'contamination': 0.005, 'support_fraction': 0.7}\n",
      "{'contamination': 0.005, 'support_fraction': 0.8}\n",
      "{'contamination': 0.005, 'support_fraction': 0.9}\n",
      "{'contamination': 0.01, 'support_fraction': 0.6}\n",
      "{'contamination': 0.01, 'support_fraction': 0.7}\n",
      "{'contamination': 0.01, 'support_fraction': 0.8}\n",
      "{'contamination': 0.01, 'support_fraction': 0.9}\n",
      "{'contamination': 0.1, 'support_fraction': 0.6}\n",
      "{'contamination': 0.1, 'support_fraction': 0.7}\n",
      "{'contamination': 0.1, 'support_fraction': 0.8}\n",
      "{'contamination': 0.1, 'support_fraction': 0.9}\n",
      "=========== DONE ==========\n",
      "{'contamination': 0.001, 'support_fraction': 0.6, 'f1': 0.5625, 'recall': 0.46153846153846156, 'precision': 0.72}\n",
      "\n",
      "{'contamination': 0.1, 'support_fraction': 0.9, 'f1': 0.026782197715636075, 'recall': 0.8717948717948718, 'precision': 0.0136}\n",
      "\n",
      "{'contamination': 0.001, 'support_fraction': 0.6, 'f1': 0.5625, 'recall': 0.46153846153846156, 'precision': 0.72}\n"
     ]
    }
   ],
   "source": [
    "train = get_sample(deskewed, 25000)\n",
    "test = train.copy()\n",
    "\n",
    "params = {\n",
    "    'contamination': [0.001,0.005, 0.01, 0.1],\n",
    "    'support_fraction': [0.6, 0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "default_params = {\n",
    "    'assume_centered': True,\n",
    "    'random_state': SEED\n",
    "}\n",
    "\n",
    "best_f1, best_recall, best_precision = outliers_grid(EllipticEnvelope, train, test, default_params, **params)\n",
    "print(best_f1)\n",
    "print()\n",
    "print(best_recall)\n",
    "print()\n",
    "print(best_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on whole dataset to generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5070785070785072\n",
      "0.40040650406504064\n",
      "0.6912280701754386\n"
     ]
    }
   ],
   "source": [
    "x = deskewed[sub_cols]\n",
    "y = df.Class\n",
    "contamination = best_f1['contamination'] # 0.001\n",
    "support_frac = best_f1['support_fraction'] # 0.7\n",
    "\n",
    "ee = EllipticEnvelope(assume_centered=True, contamination=contamination, support_fraction=support_frac,random_state=SEED)\n",
    "ee.fit(x,y)\n",
    "ypred = np.where(ee.predict(x)>0, 0, 1)\n",
    "print(metrics.f1_score(y,ypred))\n",
    "print(metrics.recall_score(y,ypred))\n",
    "print(metrics.precision_score(y,ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the Envelope on _only_ normal charges seems to improve the model even more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6037735849056605\n",
      "0.6829268292682927\n",
      "0.5410628019323671\n"
     ]
    }
   ],
   "source": [
    "x = deskewed.loc[deskewed.Class==0, sub_cols].copy()\n",
    "\n",
    "ee = EllipticEnvelope(assume_centered=True, contamination=contamination, support_fraction=support_frac,random_state=SEED)\n",
    "ee.fit(x)\n",
    "ypred = np.where(ee.predict(deskewed[sub_cols])>0, 0, 1)\n",
    "print(metrics.f1_score(y,ypred))\n",
    "print(metrics.recall_score(y,ypred))\n",
    "print(metrics.precision_score(y,ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use predictions as features, or maybe even better would be to feed in Mahalanobis distances from best mcd. Or we can create a Multivariate Normal Distribution and use it's pdf to generate features for rows. Although I believe this is the same as the Mahalanobis distance just scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save mahalanobis dists for ensemble models\n",
    "outlier_ftr_df['robust_mahalanobis_dists'] = ee.mahalanobis(deskewed[sub_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Outlier Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local Outlier Factor is a way of scoring data points based on their relative densities to their nearest neighbors.\n",
    "The theory is that a “normal\" data point is expected to have a similar density to it’s neighbors, while data points with lower relative density (as compared to their neighbors) are more likely to be outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See for example below, points O1, O2, and O3 are outliers, but point O4 is not even though it's _distance_ to it's neighbors is comparable to O1 and O2. However the density of O4s neighbors is _not_ comparable to the neighbors of O1 and O2.\n",
    "\n",
    "![LOFExample](https://i.stack.imgur.com/EFB37.jpg![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lof_grid(x, y, n_neighbors_opts):\n",
    "    best_f1 = {'nn':0, 'thresh':0, 'f1':0, 'recall':0, 'precision':0}\n",
    "    best_recall = {'nn':0, 'thresh':0, 'f1':0, 'recall':0, 'precision':0}\n",
    "    best_precision = {'nn':0, 'thresh':0, 'f1':0, 'recall':0, 'precision':0}\n",
    "    \n",
    "    for nn in n_neighbors:\n",
    "        lof = LocalOutlierFactor(nn)\n",
    "        lof.fit(x,y)\n",
    "        for thresh in np.arange(-1,-3, -0.2):\n",
    "            ypred = np.where(lof.negative_outlier_factor_ < thresh, 1, 0)\n",
    "            f1 = metrics.f1_score(y,ypred)\n",
    "            recall = metrics.recall_score(y,ypred)\n",
    "            precision = metrics.precision_score(y,ypred)\n",
    "            if f1 > best_f1['f1']:\n",
    "                best_f1 = {'nn':nn, 'thresh':thresh, 'f1':f1, 'recall':recall, 'precision':precision}\n",
    "            if recall > best_recall['recall']:\n",
    "                best_recall = {'nn':nn, 'thresh':thresh, 'f1':f1, 'recall':recall, 'precision':precision}\n",
    "            if precision > best_precision['precision']:\n",
    "                best_precision = {'nn':nn, 'thresh':thresh, 'f1':f1, 'recall':recall, 'precision':precision}\n",
    "    return best_f1, best_recall, best_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance Based methods are very costly, so performing the rest on sub sample of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    24750\n",
      "1      250\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Interestingly, this algorithm performs way better on _unscaled_ data\n",
    "# This is very strange and suggests that dollar amount is a far more important\n",
    "# factor than the other variables\n",
    "samp = get_sample(df, (24750, 250))\n",
    "print(samp.Class.value_counts())\n",
    "\n",
    "x = samp[sub_cols]\n",
    "y = samp.Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nn': 230, 'thresh': -2.3999999999999995, 'f1': 0.5206463195691203, 'recall': 0.58, 'precision': 0.4723127035830619}\n",
      "\n",
      "{'nn': 275, 'thresh': -1.0, 'f1': 0.02606997730750963, 'recall': 0.988, 'precision': 0.013209262527407882}\n",
      "\n",
      "{'nn': 230, 'thresh': -2.8, 'f1': 0.5147679324894514, 'recall': 0.488, 'precision': 0.5446428571428571}\n"
     ]
    }
   ],
   "source": [
    "n_neighbors = np.arange(230,300, 5)\n",
    "best_f1, best_recall, best_precision = lof_grid(x,y, n_neighbors)\n",
    "\n",
    "print(best_f1)\n",
    "print()\n",
    "print(best_recall)\n",
    "print()\n",
    "print(best_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresh: -1.0\n",
      "0.004134803106643818\n",
      "0.9857723577235772\n",
      "0.0020717465036607973\n",
      "\n",
      "Thresh: -1.2\n",
      "0.014387288446773708\n",
      "0.8760162601626016\n",
      "0.00725320588334287\n",
      "\n",
      "Thresh: -1.4\n",
      "0.04571935859734783\n",
      "0.8373983739837398\n",
      "0.023501226398950432\n",
      "\n",
      "Thresh: -1.5999999999999999\n",
      "0.11535125758889853\n",
      "0.8109756097560976\n",
      "0.06209150326797386\n",
      "\n",
      "Thresh: -1.7999999999999998\n",
      "0.19609967497291442\n",
      "0.7357723577235772\n",
      "0.113125\n",
      "\n",
      "Thresh: -1.9999999999999998\n",
      "0.2601828761429759\n",
      "0.6361788617886179\n",
      "0.16353187042842215\n",
      "\n",
      "Thresh: -2.1999999999999997\n",
      "0.2847222222222222\n",
      "0.5\n",
      "0.19902912621359223\n",
      "\n",
      "Thresh: -2.3999999999999995\n",
      "0.3037417461482025\n",
      "0.42073170731707316\n",
      "0.23765786452353616\n",
      "\n",
      "Thresh: -2.5999999999999996\n",
      "0.2857142857142857\n",
      "0.32926829268292684\n",
      "0.2523364485981308\n",
      "\n",
      "Thresh: -2.8\n",
      "0.27281845536609833\n",
      "0.2764227642276423\n",
      "0.2693069306930693\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on full dataset\n",
    "x = df[sub_cols]\n",
    "y = df.Class\n",
    "\n",
    "lof = LocalOutlierFactor(best_f1['nn']) # use best f1 params\n",
    "lof.fit(x)\n",
    "for thresh in np.arange(-1,-3, -0.2):\n",
    "    print('Thresh:', thresh)\n",
    "    ypred = np.where(lof.negative_outlier_factor_ < thresh, 1, 0)\n",
    "    f1 = metrics.f1_score(y,ypred)\n",
    "    recall = metrics.recall_score(y,ypred)\n",
    "    precision = metrics.precision_score(y,ypred)\n",
    "    print(f1)\n",
    "    print(recall)\n",
    "    print(precision)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresh: -1.0\n",
      "0.004134803106643818\n",
      "0.9857723577235772\n",
      "0.0020717465036607973\n",
      "\n",
      "Thresh: -1.2\n",
      "0.014387288446773708\n",
      "0.8760162601626016\n",
      "0.00725320588334287\n",
      "\n",
      "Thresh: -1.4\n",
      "0.04571935859734783\n",
      "0.8373983739837398\n",
      "0.023501226398950432\n",
      "\n",
      "Thresh: -1.5999999999999999\n",
      "0.11535125758889853\n",
      "0.8109756097560976\n",
      "0.06209150326797386\n",
      "\n",
      "Thresh: -1.7999999999999998\n",
      "0.19609967497291442\n",
      "0.7357723577235772\n",
      "0.113125\n",
      "\n",
      "Thresh: -1.9999999999999998\n",
      "0.2601828761429759\n",
      "0.6361788617886179\n",
      "0.16353187042842215\n",
      "\n",
      "Thresh: -2.1999999999999997\n",
      "0.2847222222222222\n",
      "0.5\n",
      "0.19902912621359223\n",
      "\n",
      "Thresh: -2.3999999999999995\n",
      "0.3037417461482025\n",
      "0.42073170731707316\n",
      "0.23765786452353616\n",
      "\n",
      "Thresh: -2.5999999999999996\n",
      "0.2857142857142857\n",
      "0.32926829268292684\n",
      "0.2523364485981308\n",
      "\n",
      "Thresh: -2.8\n",
      "0.27281845536609833\n",
      "0.2764227642276423\n",
      "0.2693069306930693\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on full dataset pt2\n",
    "x = df[sub_cols]\n",
    "y = df.Class\n",
    "\n",
    "lof = LocalOutlierFactor(best_precision['nn']) # use best precision params\n",
    "lof.fit(x)\n",
    "for thresh in np.arange(-1,-3, -0.2):\n",
    "    print('Thresh:', thresh)\n",
    "    ypred = np.where(lof.negative_outlier_factor_ < thresh, 1, 0)\n",
    "    f1 = metrics.f1_score(y,ypred)\n",
    "    recall = metrics.recall_score(y,ypred)\n",
    "    precision = metrics.precision_score(y,ypred)\n",
    "    print(f1)\n",
    "    print(recall)\n",
    "    print(precision)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocalOutlierFactor(algorithm='auto', contamination=0.1, leaf_size=30,\n",
       "          metric='minkowski', metric_params=None, n_jobs=1,\n",
       "          n_neighbors=295, p=2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So best params are\n",
    "nn = 295\n",
    "thresh = -2.4 \n",
    "lof = LocalOutlierFactor(nn)\n",
    "lof.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save LOF score as feature\n",
    "outlier_ftr_df['lof'] = lof.negative_outlier_factor_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators | max_samples | max_features | contamination\n",
      "-----------------------------\n",
      "{'n_estimators': 250, 'max_samples': 0.6, 'max_features': 0.2, 'contamination': 0.0005}\n",
      "{'n_estimators': 250, 'max_samples': 0.6, 'max_features': 0.2, 'contamination': 0.001}\n",
      "{'n_estimators': 250, 'max_samples': 0.6, 'max_features': 0.2, 'contamination': 0.005}\n",
      "{'n_estimators': 250, 'max_samples': 0.6, 'max_features': 0.2, 'contamination': 0.01}\n",
      "{'n_estimators': 250, 'max_samples': 0.6, 'max_features': 0.25, 'contamination': 0.0005}\n",
      "{'n_estimators': 250, 'max_samples': 0.6, 'max_features': 0.25, 'contamination': 0.001}\n",
      "{'n_estimators': 250, 'max_samples': 0.6, 'max_features': 0.25, 'contamination': 0.005}\n",
      "{'n_estimators': 250, 'max_samples': 0.6, 'max_features': 0.25, 'contamination': 0.01}\n",
      "{'n_estimators': 250, 'max_samples': 0.6, 'max_features': 0.3, 'contamination': 0.0005}\n",
      "{'n_estimators': 250, 'max_samples': 0.6, 'max_features': 0.3, 'contamination': 0.001}\n",
      "{'n_estimators': 250, 'max_samples': 0.6, 'max_features': 0.3, 'contamination': 0.005}\n",
      "{'n_estimators': 250, 'max_samples': 0.6, 'max_features': 0.3, 'contamination': 0.01}\n",
      "{'n_estimators': 250, 'max_samples': 0.6, 'max_features': 0.4, 'contamination': 0.0005}\n",
      "{'n_estimators': 250, 'max_samples': 0.6, 'max_features': 0.4, 'contamination': 0.001}\n",
      "{'n_estimators': 250, 'max_samples': 0.6, 'max_features': 0.4, 'contamination': 0.005}\n",
      "{'n_estimators': 250, 'max_samples': 0.6, 'max_features': 0.4, 'contamination': 0.01}\n",
      "{'n_estimators': 250, 'max_samples': 0.7, 'max_features': 0.2, 'contamination': 0.0005}\n",
      "{'n_estimators': 250, 'max_samples': 0.7, 'max_features': 0.2, 'contamination': 0.001}\n",
      "{'n_estimators': 250, 'max_samples': 0.7, 'max_features': 0.2, 'contamination': 0.005}\n",
      "{'n_estimators': 250, 'max_samples': 0.7, 'max_features': 0.2, 'contamination': 0.01}\n",
      "{'n_estimators': 250, 'max_samples': 0.7, 'max_features': 0.25, 'contamination': 0.0005}\n",
      "{'n_estimators': 250, 'max_samples': 0.7, 'max_features': 0.25, 'contamination': 0.001}\n",
      "{'n_estimators': 250, 'max_samples': 0.7, 'max_features': 0.25, 'contamination': 0.005}\n",
      "{'n_estimators': 250, 'max_samples': 0.7, 'max_features': 0.25, 'contamination': 0.01}\n",
      "{'n_estimators': 250, 'max_samples': 0.7, 'max_features': 0.3, 'contamination': 0.0005}\n",
      "{'n_estimators': 250, 'max_samples': 0.7, 'max_features': 0.3, 'contamination': 0.001}\n",
      "{'n_estimators': 250, 'max_samples': 0.7, 'max_features': 0.3, 'contamination': 0.005}\n",
      "{'n_estimators': 250, 'max_samples': 0.7, 'max_features': 0.3, 'contamination': 0.01}\n",
      "{'n_estimators': 250, 'max_samples': 0.7, 'max_features': 0.4, 'contamination': 0.0005}\n",
      "{'n_estimators': 250, 'max_samples': 0.7, 'max_features': 0.4, 'contamination': 0.001}\n",
      "{'n_estimators': 250, 'max_samples': 0.7, 'max_features': 0.4, 'contamination': 0.005}\n",
      "{'n_estimators': 250, 'max_samples': 0.7, 'max_features': 0.4, 'contamination': 0.01}\n",
      "{'n_estimators': 250, 'max_samples': 0.75, 'max_features': 0.2, 'contamination': 0.0005}\n",
      "{'n_estimators': 250, 'max_samples': 0.75, 'max_features': 0.2, 'contamination': 0.001}\n",
      "{'n_estimators': 250, 'max_samples': 0.75, 'max_features': 0.2, 'contamination': 0.005}\n",
      "{'n_estimators': 250, 'max_samples': 0.75, 'max_features': 0.2, 'contamination': 0.01}\n",
      "{'n_estimators': 250, 'max_samples': 0.75, 'max_features': 0.25, 'contamination': 0.0005}\n",
      "{'n_estimators': 250, 'max_samples': 0.75, 'max_features': 0.25, 'contamination': 0.001}\n",
      "{'n_estimators': 250, 'max_samples': 0.75, 'max_features': 0.25, 'contamination': 0.005}\n",
      "{'n_estimators': 250, 'max_samples': 0.75, 'max_features': 0.25, 'contamination': 0.01}\n",
      "{'n_estimators': 250, 'max_samples': 0.75, 'max_features': 0.3, 'contamination': 0.0005}\n",
      "{'n_estimators': 250, 'max_samples': 0.75, 'max_features': 0.3, 'contamination': 0.001}\n",
      "{'n_estimators': 250, 'max_samples': 0.75, 'max_features': 0.3, 'contamination': 0.005}\n",
      "{'n_estimators': 250, 'max_samples': 0.75, 'max_features': 0.3, 'contamination': 0.01}\n",
      "{'n_estimators': 250, 'max_samples': 0.75, 'max_features': 0.4, 'contamination': 0.0005}\n",
      "{'n_estimators': 250, 'max_samples': 0.75, 'max_features': 0.4, 'contamination': 0.001}\n",
      "{'n_estimators': 250, 'max_samples': 0.75, 'max_features': 0.4, 'contamination': 0.005}\n",
      "{'n_estimators': 250, 'max_samples': 0.75, 'max_features': 0.4, 'contamination': 0.01}\n",
      "{'n_estimators': 250, 'max_samples': 0.8, 'max_features': 0.2, 'contamination': 0.0005}\n",
      "{'n_estimators': 250, 'max_samples': 0.8, 'max_features': 0.2, 'contamination': 0.001}\n",
      "{'n_estimators': 250, 'max_samples': 0.8, 'max_features': 0.2, 'contamination': 0.005}\n",
      "{'n_estimators': 250, 'max_samples': 0.8, 'max_features': 0.2, 'contamination': 0.01}\n",
      "{'n_estimators': 250, 'max_samples': 0.8, 'max_features': 0.25, 'contamination': 0.0005}\n",
      "{'n_estimators': 250, 'max_samples': 0.8, 'max_features': 0.25, 'contamination': 0.001}\n",
      "{'n_estimators': 250, 'max_samples': 0.8, 'max_features': 0.25, 'contamination': 0.005}\n",
      "{'n_estimators': 250, 'max_samples': 0.8, 'max_features': 0.25, 'contamination': 0.01}\n",
      "{'n_estimators': 250, 'max_samples': 0.8, 'max_features': 0.3, 'contamination': 0.0005}\n",
      "{'n_estimators': 250, 'max_samples': 0.8, 'max_features': 0.3, 'contamination': 0.001}\n",
      "{'n_estimators': 250, 'max_samples': 0.8, 'max_features': 0.3, 'contamination': 0.005}\n",
      "{'n_estimators': 250, 'max_samples': 0.8, 'max_features': 0.3, 'contamination': 0.01}\n",
      "{'n_estimators': 250, 'max_samples': 0.8, 'max_features': 0.4, 'contamination': 0.0005}\n",
      "{'n_estimators': 250, 'max_samples': 0.8, 'max_features': 0.4, 'contamination': 0.001}\n",
      "{'n_estimators': 250, 'max_samples': 0.8, 'max_features': 0.4, 'contamination': 0.005}\n",
      "{'n_estimators': 250, 'max_samples': 0.8, 'max_features': 0.4, 'contamination': 0.01}\n",
      "{'n_estimators': 300, 'max_samples': 0.6, 'max_features': 0.2, 'contamination': 0.0005}\n",
      "{'n_estimators': 300, 'max_samples': 0.6, 'max_features': 0.2, 'contamination': 0.001}\n",
      "{'n_estimators': 300, 'max_samples': 0.6, 'max_features': 0.2, 'contamination': 0.005}\n",
      "{'n_estimators': 300, 'max_samples': 0.6, 'max_features': 0.2, 'contamination': 0.01}\n",
      "{'n_estimators': 300, 'max_samples': 0.6, 'max_features': 0.25, 'contamination': 0.0005}\n",
      "{'n_estimators': 300, 'max_samples': 0.6, 'max_features': 0.25, 'contamination': 0.001}\n",
      "{'n_estimators': 300, 'max_samples': 0.6, 'max_features': 0.25, 'contamination': 0.005}\n",
      "{'n_estimators': 300, 'max_samples': 0.6, 'max_features': 0.25, 'contamination': 0.01}\n",
      "{'n_estimators': 300, 'max_samples': 0.6, 'max_features': 0.3, 'contamination': 0.0005}\n",
      "{'n_estimators': 300, 'max_samples': 0.6, 'max_features': 0.3, 'contamination': 0.001}\n",
      "{'n_estimators': 300, 'max_samples': 0.6, 'max_features': 0.3, 'contamination': 0.005}\n",
      "{'n_estimators': 300, 'max_samples': 0.6, 'max_features': 0.3, 'contamination': 0.01}\n",
      "{'n_estimators': 300, 'max_samples': 0.6, 'max_features': 0.4, 'contamination': 0.0005}\n",
      "{'n_estimators': 300, 'max_samples': 0.6, 'max_features': 0.4, 'contamination': 0.001}\n",
      "{'n_estimators': 300, 'max_samples': 0.6, 'max_features': 0.4, 'contamination': 0.005}\n",
      "{'n_estimators': 300, 'max_samples': 0.6, 'max_features': 0.4, 'contamination': 0.01}\n",
      "{'n_estimators': 300, 'max_samples': 0.7, 'max_features': 0.2, 'contamination': 0.0005}\n",
      "{'n_estimators': 300, 'max_samples': 0.7, 'max_features': 0.2, 'contamination': 0.001}\n",
      "{'n_estimators': 300, 'max_samples': 0.7, 'max_features': 0.2, 'contamination': 0.005}\n",
      "{'n_estimators': 300, 'max_samples': 0.7, 'max_features': 0.2, 'contamination': 0.01}\n",
      "{'n_estimators': 300, 'max_samples': 0.7, 'max_features': 0.25, 'contamination': 0.0005}\n",
      "{'n_estimators': 300, 'max_samples': 0.7, 'max_features': 0.25, 'contamination': 0.001}\n",
      "{'n_estimators': 300, 'max_samples': 0.7, 'max_features': 0.25, 'contamination': 0.005}\n",
      "{'n_estimators': 300, 'max_samples': 0.7, 'max_features': 0.25, 'contamination': 0.01}\n",
      "{'n_estimators': 300, 'max_samples': 0.7, 'max_features': 0.3, 'contamination': 0.0005}\n",
      "{'n_estimators': 300, 'max_samples': 0.7, 'max_features': 0.3, 'contamination': 0.001}\n",
      "{'n_estimators': 300, 'max_samples': 0.7, 'max_features': 0.3, 'contamination': 0.005}\n",
      "{'n_estimators': 300, 'max_samples': 0.7, 'max_features': 0.3, 'contamination': 0.01}\n",
      "{'n_estimators': 300, 'max_samples': 0.7, 'max_features': 0.4, 'contamination': 0.0005}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 300, 'max_samples': 0.7, 'max_features': 0.4, 'contamination': 0.001}\n",
      "{'n_estimators': 300, 'max_samples': 0.7, 'max_features': 0.4, 'contamination': 0.005}\n",
      "{'n_estimators': 300, 'max_samples': 0.7, 'max_features': 0.4, 'contamination': 0.01}\n",
      "{'n_estimators': 300, 'max_samples': 0.75, 'max_features': 0.2, 'contamination': 0.0005}\n",
      "{'n_estimators': 300, 'max_samples': 0.75, 'max_features': 0.2, 'contamination': 0.001}\n",
      "{'n_estimators': 300, 'max_samples': 0.75, 'max_features': 0.2, 'contamination': 0.005}\n",
      "{'n_estimators': 300, 'max_samples': 0.75, 'max_features': 0.2, 'contamination': 0.01}\n",
      "{'n_estimators': 300, 'max_samples': 0.75, 'max_features': 0.25, 'contamination': 0.0005}\n",
      "{'n_estimators': 300, 'max_samples': 0.75, 'max_features': 0.25, 'contamination': 0.001}\n",
      "{'n_estimators': 300, 'max_samples': 0.75, 'max_features': 0.25, 'contamination': 0.005}\n",
      "{'n_estimators': 300, 'max_samples': 0.75, 'max_features': 0.25, 'contamination': 0.01}\n",
      "{'n_estimators': 300, 'max_samples': 0.75, 'max_features': 0.3, 'contamination': 0.0005}\n",
      "{'n_estimators': 300, 'max_samples': 0.75, 'max_features': 0.3, 'contamination': 0.001}\n",
      "{'n_estimators': 300, 'max_samples': 0.75, 'max_features': 0.3, 'contamination': 0.005}\n",
      "{'n_estimators': 300, 'max_samples': 0.75, 'max_features': 0.3, 'contamination': 0.01}\n",
      "{'n_estimators': 300, 'max_samples': 0.75, 'max_features': 0.4, 'contamination': 0.0005}\n",
      "{'n_estimators': 300, 'max_samples': 0.75, 'max_features': 0.4, 'contamination': 0.001}\n",
      "{'n_estimators': 300, 'max_samples': 0.75, 'max_features': 0.4, 'contamination': 0.005}\n",
      "{'n_estimators': 300, 'max_samples': 0.75, 'max_features': 0.4, 'contamination': 0.01}\n",
      "{'n_estimators': 300, 'max_samples': 0.8, 'max_features': 0.2, 'contamination': 0.0005}\n",
      "{'n_estimators': 300, 'max_samples': 0.8, 'max_features': 0.2, 'contamination': 0.001}\n",
      "{'n_estimators': 300, 'max_samples': 0.8, 'max_features': 0.2, 'contamination': 0.005}\n",
      "{'n_estimators': 300, 'max_samples': 0.8, 'max_features': 0.2, 'contamination': 0.01}\n",
      "{'n_estimators': 300, 'max_samples': 0.8, 'max_features': 0.25, 'contamination': 0.0005}\n",
      "{'n_estimators': 300, 'max_samples': 0.8, 'max_features': 0.25, 'contamination': 0.001}\n",
      "{'n_estimators': 300, 'max_samples': 0.8, 'max_features': 0.25, 'contamination': 0.005}\n",
      "{'n_estimators': 300, 'max_samples': 0.8, 'max_features': 0.25, 'contamination': 0.01}\n",
      "{'n_estimators': 300, 'max_samples': 0.8, 'max_features': 0.3, 'contamination': 0.0005}\n",
      "{'n_estimators': 300, 'max_samples': 0.8, 'max_features': 0.3, 'contamination': 0.001}\n",
      "{'n_estimators': 300, 'max_samples': 0.8, 'max_features': 0.3, 'contamination': 0.005}\n",
      "{'n_estimators': 300, 'max_samples': 0.8, 'max_features': 0.3, 'contamination': 0.01}\n",
      "{'n_estimators': 300, 'max_samples': 0.8, 'max_features': 0.4, 'contamination': 0.0005}\n",
      "{'n_estimators': 300, 'max_samples': 0.8, 'max_features': 0.4, 'contamination': 0.001}\n",
      "{'n_estimators': 300, 'max_samples': 0.8, 'max_features': 0.4, 'contamination': 0.005}\n",
      "{'n_estimators': 300, 'max_samples': 0.8, 'max_features': 0.4, 'contamination': 0.01}\n",
      "{'n_estimators': 325, 'max_samples': 0.6, 'max_features': 0.2, 'contamination': 0.0005}\n",
      "{'n_estimators': 325, 'max_samples': 0.6, 'max_features': 0.2, 'contamination': 0.001}\n",
      "{'n_estimators': 325, 'max_samples': 0.6, 'max_features': 0.2, 'contamination': 0.005}\n",
      "{'n_estimators': 325, 'max_samples': 0.6, 'max_features': 0.2, 'contamination': 0.01}\n",
      "{'n_estimators': 325, 'max_samples': 0.6, 'max_features': 0.25, 'contamination': 0.0005}\n",
      "{'n_estimators': 325, 'max_samples': 0.6, 'max_features': 0.25, 'contamination': 0.001}\n",
      "{'n_estimators': 325, 'max_samples': 0.6, 'max_features': 0.25, 'contamination': 0.005}\n",
      "{'n_estimators': 325, 'max_samples': 0.6, 'max_features': 0.25, 'contamination': 0.01}\n",
      "{'n_estimators': 325, 'max_samples': 0.6, 'max_features': 0.3, 'contamination': 0.0005}\n",
      "{'n_estimators': 325, 'max_samples': 0.6, 'max_features': 0.3, 'contamination': 0.001}\n",
      "{'n_estimators': 325, 'max_samples': 0.6, 'max_features': 0.3, 'contamination': 0.005}\n",
      "{'n_estimators': 325, 'max_samples': 0.6, 'max_features': 0.3, 'contamination': 0.01}\n",
      "{'n_estimators': 325, 'max_samples': 0.6, 'max_features': 0.4, 'contamination': 0.0005}\n",
      "{'n_estimators': 325, 'max_samples': 0.6, 'max_features': 0.4, 'contamination': 0.001}\n",
      "{'n_estimators': 325, 'max_samples': 0.6, 'max_features': 0.4, 'contamination': 0.005}\n",
      "{'n_estimators': 325, 'max_samples': 0.6, 'max_features': 0.4, 'contamination': 0.01}\n",
      "{'n_estimators': 325, 'max_samples': 0.7, 'max_features': 0.2, 'contamination': 0.0005}\n",
      "{'n_estimators': 325, 'max_samples': 0.7, 'max_features': 0.2, 'contamination': 0.001}\n",
      "{'n_estimators': 325, 'max_samples': 0.7, 'max_features': 0.2, 'contamination': 0.005}\n",
      "{'n_estimators': 325, 'max_samples': 0.7, 'max_features': 0.2, 'contamination': 0.01}\n",
      "{'n_estimators': 325, 'max_samples': 0.7, 'max_features': 0.25, 'contamination': 0.0005}\n",
      "{'n_estimators': 325, 'max_samples': 0.7, 'max_features': 0.25, 'contamination': 0.001}\n",
      "{'n_estimators': 325, 'max_samples': 0.7, 'max_features': 0.25, 'contamination': 0.005}\n",
      "{'n_estimators': 325, 'max_samples': 0.7, 'max_features': 0.25, 'contamination': 0.01}\n",
      "{'n_estimators': 325, 'max_samples': 0.7, 'max_features': 0.3, 'contamination': 0.0005}\n",
      "{'n_estimators': 325, 'max_samples': 0.7, 'max_features': 0.3, 'contamination': 0.001}\n",
      "{'n_estimators': 325, 'max_samples': 0.7, 'max_features': 0.3, 'contamination': 0.005}\n",
      "{'n_estimators': 325, 'max_samples': 0.7, 'max_features': 0.3, 'contamination': 0.01}\n",
      "{'n_estimators': 325, 'max_samples': 0.7, 'max_features': 0.4, 'contamination': 0.0005}\n",
      "{'n_estimators': 325, 'max_samples': 0.7, 'max_features': 0.4, 'contamination': 0.001}\n",
      "{'n_estimators': 325, 'max_samples': 0.7, 'max_features': 0.4, 'contamination': 0.005}\n",
      "{'n_estimators': 325, 'max_samples': 0.7, 'max_features': 0.4, 'contamination': 0.01}\n",
      "{'n_estimators': 325, 'max_samples': 0.75, 'max_features': 0.2, 'contamination': 0.0005}\n",
      "{'n_estimators': 325, 'max_samples': 0.75, 'max_features': 0.2, 'contamination': 0.001}\n",
      "{'n_estimators': 325, 'max_samples': 0.75, 'max_features': 0.2, 'contamination': 0.005}\n",
      "{'n_estimators': 325, 'max_samples': 0.75, 'max_features': 0.2, 'contamination': 0.01}\n",
      "{'n_estimators': 325, 'max_samples': 0.75, 'max_features': 0.25, 'contamination': 0.0005}\n",
      "{'n_estimators': 325, 'max_samples': 0.75, 'max_features': 0.25, 'contamination': 0.001}\n",
      "{'n_estimators': 325, 'max_samples': 0.75, 'max_features': 0.25, 'contamination': 0.005}\n",
      "{'n_estimators': 325, 'max_samples': 0.75, 'max_features': 0.25, 'contamination': 0.01}\n",
      "{'n_estimators': 325, 'max_samples': 0.75, 'max_features': 0.3, 'contamination': 0.0005}\n",
      "{'n_estimators': 325, 'max_samples': 0.75, 'max_features': 0.3, 'contamination': 0.001}\n",
      "{'n_estimators': 325, 'max_samples': 0.75, 'max_features': 0.3, 'contamination': 0.005}\n",
      "{'n_estimators': 325, 'max_samples': 0.75, 'max_features': 0.3, 'contamination': 0.01}\n",
      "{'n_estimators': 325, 'max_samples': 0.75, 'max_features': 0.4, 'contamination': 0.0005}\n",
      "{'n_estimators': 325, 'max_samples': 0.75, 'max_features': 0.4, 'contamination': 0.001}\n",
      "{'n_estimators': 325, 'max_samples': 0.75, 'max_features': 0.4, 'contamination': 0.005}\n",
      "{'n_estimators': 325, 'max_samples': 0.75, 'max_features': 0.4, 'contamination': 0.01}\n",
      "{'n_estimators': 325, 'max_samples': 0.8, 'max_features': 0.2, 'contamination': 0.0005}\n",
      "{'n_estimators': 325, 'max_samples': 0.8, 'max_features': 0.2, 'contamination': 0.001}\n",
      "{'n_estimators': 325, 'max_samples': 0.8, 'max_features': 0.2, 'contamination': 0.005}\n",
      "{'n_estimators': 325, 'max_samples': 0.8, 'max_features': 0.2, 'contamination': 0.01}\n",
      "{'n_estimators': 325, 'max_samples': 0.8, 'max_features': 0.25, 'contamination': 0.0005}\n",
      "{'n_estimators': 325, 'max_samples': 0.8, 'max_features': 0.25, 'contamination': 0.001}\n",
      "{'n_estimators': 325, 'max_samples': 0.8, 'max_features': 0.25, 'contamination': 0.005}\n",
      "{'n_estimators': 325, 'max_samples': 0.8, 'max_features': 0.25, 'contamination': 0.01}\n",
      "{'n_estimators': 325, 'max_samples': 0.8, 'max_features': 0.3, 'contamination': 0.0005}\n",
      "{'n_estimators': 325, 'max_samples': 0.8, 'max_features': 0.3, 'contamination': 0.001}\n",
      "{'n_estimators': 325, 'max_samples': 0.8, 'max_features': 0.3, 'contamination': 0.005}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 325, 'max_samples': 0.8, 'max_features': 0.3, 'contamination': 0.01}\n",
      "{'n_estimators': 325, 'max_samples': 0.8, 'max_features': 0.4, 'contamination': 0.0005}\n",
      "{'n_estimators': 325, 'max_samples': 0.8, 'max_features': 0.4, 'contamination': 0.001}\n",
      "{'n_estimators': 325, 'max_samples': 0.8, 'max_features': 0.4, 'contamination': 0.005}\n",
      "{'n_estimators': 325, 'max_samples': 0.8, 'max_features': 0.4, 'contamination': 0.01}\n",
      "=========== DONE ==========\n",
      "{'n_estimators': 250, 'max_samples': 0.6, 'max_features': 0.4, 'contamination': 0.0005, 'f1': 0.33333333333333337, 'recall': 0.24324324324324326, 'precision': 0.5294117647058824}\n",
      "\n",
      "{'n_estimators': 250, 'max_samples': 0.6, 'max_features': 0.2, 'contamination': 0.01, 'f1': 0.1506849315068493, 'recall': 0.5945945945945946, 'precision': 0.08627450980392157}\n",
      "\n",
      "{'n_estimators': 250, 'max_samples': 0.6, 'max_features': 0.4, 'contamination': 0.0005, 'f1': 0.33333333333333337, 'recall': 0.24324324324324326, 'precision': 0.5294117647058824}\n"
     ]
    }
   ],
   "source": [
    "train = get_sample(df, 25000)\n",
    "test = get_sample(df[~df.index.isin(train.index)], 25000)\n",
    "\n",
    "\n",
    "n_est = [250, 300, 325]\n",
    "max_samples = [0.6, 0.7, 0.75, 0.8]\n",
    "max_ftrs = [0.2, 0.25, 0.3, 0.4]\n",
    "contam = [0.0005, 0.001, 0.005, 0.01]\n",
    "best_f1, best_recall, best_precision = outliers_grid(IsolationForest, train, test, \n",
    "                                                     n_estimators=n_est, \n",
    "                                                     max_samples=max_samples, \n",
    "                                                     max_features=max_ftrs, \n",
    "                                                     contamination=contam)\n",
    "print(best_f1)\n",
    "print()\n",
    "print(best_recall)\n",
    "print()\n",
    "print(best_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the forest on a small sub-sample of only normal data seems to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators | max_samples | max_features | contamination\n",
      "-----------------------------\n",
      "{'n_estimators': 300, 'max_samples': 0.7, 'max_features': 0.3, 'contamination': 0.0005}\n",
      "{'n_estimators': 300, 'max_samples': 0.7, 'max_features': 0.3, 'contamination': 0.001}\n",
      "{'n_estimators': 300, 'max_samples': 0.7, 'max_features': 0.3, 'contamination': 0.005}\n",
      "{'n_estimators': 300, 'max_samples': 0.7, 'max_features': 0.3, 'contamination': 0.01}\n",
      "{'n_estimators': 325, 'max_samples': 0.7, 'max_features': 0.3, 'contamination': 0.0005}\n",
      "{'n_estimators': 325, 'max_samples': 0.7, 'max_features': 0.3, 'contamination': 0.001}\n",
      "{'n_estimators': 325, 'max_samples': 0.7, 'max_features': 0.3, 'contamination': 0.005}\n",
      "{'n_estimators': 325, 'max_samples': 0.7, 'max_features': 0.3, 'contamination': 0.01}\n",
      "=========== DONE ==========\n",
      "{'n_estimators': 300, 'max_samples': 0.7, 'max_features': 0.3, 'contamination': 0.001, 'f1': 0.4444444444444444, 'recall': 0.45614035087719296, 'precision': 0.43333333333333335}\n",
      "\n",
      "{'n_estimators': 300, 'max_samples': 0.7, 'max_features': 0.3, 'contamination': 0.01, 'f1': 0.2446808510638298, 'recall': 0.8070175438596491, 'precision': 0.14420062695924765}\n",
      "\n",
      "{'n_estimators': 325, 'max_samples': 0.7, 'max_features': 0.3, 'contamination': 0.0005, 'f1': 0.4086021505376344, 'recall': 0.3333333333333333, 'precision': 0.5277777777777778}\n"
     ]
    }
   ],
   "source": [
    "train = get_sample(df, (25000, 0))\n",
    "test = get_sample(df[~df.index.isin(train.index)], 25000)\n",
    "\n",
    "n_est = [300, 325]\n",
    "max_samples = [0.7]\n",
    "max_ftrs = [0.3]\n",
    "contam = [0.0005, 0.001, 0.005, 0.01]\n",
    "\n",
    "\n",
    "# Seems to do much better!\n",
    "best_f1, best_recall, best_precision = outliers_grid(IsolationForest, train, test, \n",
    "                                                     n_estimators=n_est, \n",
    "                                                     max_samples=max_samples, \n",
    "                                                     max_features=max_ftrs, \n",
    "                                                     contamination=contam)\n",
    "print(best_f1)\n",
    "print()\n",
    "print(best_recall)\n",
    "print()\n",
    "print(best_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run Through Whole DS\n",
    "train = get_sample(df, (20000, 0))\n",
    "test = df\n",
    "\n",
    "xtrain = train[sub_cols]\n",
    "xtest = test[sub_cols]\n",
    "ytest = test.Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.3311258278145695\n",
      "Recall: 0.3556910569105691\n",
      "Precision: 0.30973451327433627\n"
     ]
    }
   ],
   "source": [
    "n_estimators = best_f1['n_estimators'] # ~ 300\n",
    "ms = best_f1['max_samples'] # ~0.7\n",
    "mf = best_f1['max_features'] # ~ 0.3\n",
    "c = best_f1['contamination'] # ~ 0.001\n",
    "\n",
    "\n",
    "isf = IsolationForest(n_estimators, max_samples=ms, max_features=mf, contamination=c)\n",
    "isf.fit(xtrain)\n",
    "preds = isf.predict(xtest)\n",
    "ypred = np.where(preds< 0, 1, 0)\n",
    "f1 = metrics.f1_score(ytest,ypred)\n",
    "recall = metrics.recall_score(ytest,ypred)\n",
    "precision = metrics.precision_score(ytest,ypred)\n",
    "\n",
    "print('F1:', f1)\n",
    "print('Recall:', recall)\n",
    "print('Precision:', precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outlier_ftr_df['iso_forest'] = ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = get_sample(df, (50000,0))\n",
    "test = get_sample(df[~df.index.isin(train.index)], 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nu | gamma\n",
      "-----------------------------\n",
      "{'nu': 0.0001, 'gamma': 1e-05}\n",
      "{'nu': 0.0001, 'gamma': 5e-05}\n",
      "{'nu': 0.0001, 'gamma': 0.0001}\n",
      "{'nu': 0.0005, 'gamma': 1e-05}\n",
      "{'nu': 0.0005, 'gamma': 5e-05}\n",
      "{'nu': 0.0005, 'gamma': 0.0001}\n",
      "{'nu': 0.001, 'gamma': 1e-05}\n",
      "{'nu': 0.001, 'gamma': 5e-05}\n",
      "{'nu': 0.001, 'gamma': 0.0001}\n",
      "{'nu': 0.005, 'gamma': 1e-05}\n",
      "{'nu': 0.005, 'gamma': 5e-05}\n",
      "{'nu': 0.005, 'gamma': 0.0001}\n",
      "=========== DONE ==========\n",
      "{'nu': 0.001, 'gamma': 1e-05, 'f1': 0.2737642585551331, 'recall': 0.3302752293577982, 'precision': 0.23376623376623376}\n",
      "\n",
      "{'nu': 0.005, 'gamma': 0.0001, 'f1': 0.22920517560073939, 'recall': 0.5688073394495413, 'precision': 0.14351851851851852}\n",
      "\n",
      "{'nu': 0.001, 'gamma': 1e-05, 'f1': 0.2737642585551331, 'recall': 0.3302752293577982, 'precision': 0.23376623376623376}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'nu': [0.0001, 0.0005, 0.001, 0.005],\n",
    "    'gamma': [0.00001, 0.00005, 0.0001],\n",
    "}\n",
    "\n",
    "default_params = {\n",
    "    'random_state':SEED\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "best_f1, best_recall, best_precision = outliers_grid(OneClassSVM, train, test, default_params, **params)\n",
    "print(best_f1)\n",
    "print()\n",
    "print(best_recall)\n",
    "print()\n",
    "print(best_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on Scaled and Deskewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scaled\n",
    "train = get_sample(scaled_df, (50000,0))\n",
    "test = get_sample(scaled_df[~scaled_df.index.isin(train.index)], 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nu | gamma\n",
      "-----------------------------\n",
      "{'nu': 0.00075, 'gamma': 0.1}\n",
      "{'nu': 0.00075, 'gamma': 0.25}\n",
      "{'nu': 0.00075, 'gamma': 0.5}\n",
      "{'nu': 0.00075, 'gamma': 0.6}\n",
      "{'nu': 0.00075, 'gamma': 0.7}\n",
      "{'nu': 0.00075, 'gamma': 0.75}\n",
      "{'nu': 0.00075, 'gamma': 0.8}\n",
      "{'nu': 0.001, 'gamma': 0.1}\n",
      "{'nu': 0.001, 'gamma': 0.25}\n",
      "{'nu': 0.001, 'gamma': 0.5}\n",
      "{'nu': 0.001, 'gamma': 0.6}\n",
      "{'nu': 0.001, 'gamma': 0.7}\n",
      "{'nu': 0.001, 'gamma': 0.75}\n",
      "{'nu': 0.001, 'gamma': 0.8}\n",
      "{'nu': 0.0025, 'gamma': 0.1}\n",
      "{'nu': 0.0025, 'gamma': 0.25}\n",
      "{'nu': 0.0025, 'gamma': 0.5}\n",
      "{'nu': 0.0025, 'gamma': 0.6}\n",
      "{'nu': 0.0025, 'gamma': 0.7}\n",
      "{'nu': 0.0025, 'gamma': 0.75}\n",
      "{'nu': 0.0025, 'gamma': 0.8}\n",
      "{'nu': 0.005, 'gamma': 0.1}\n",
      "{'nu': 0.005, 'gamma': 0.25}\n",
      "{'nu': 0.005, 'gamma': 0.5}\n",
      "{'nu': 0.005, 'gamma': 0.6}\n",
      "{'nu': 0.005, 'gamma': 0.7}\n",
      "{'nu': 0.005, 'gamma': 0.75}\n",
      "{'nu': 0.005, 'gamma': 0.8}\n",
      "=========== DONE ==========\n",
      "{'nu': 0.001, 'gamma': 0.75, 'f1': 0.5116279069767442, 'recall': 0.6055045871559633, 'precision': 0.4429530201342282}\n",
      "\n",
      "{'nu': 0.005, 'gamma': 0.1, 'f1': 0.3517382413087935, 'recall': 0.7889908256880734, 'precision': 0.22631578947368422}\n",
      "\n",
      "{'nu': 0.00075, 'gamma': 0.8, 'f1': 0.502127659574468, 'recall': 0.5412844036697247, 'precision': 0.46825396825396826}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'nu': [0.00075, 0.001, 0.0025, 0.005],\n",
    "    'gamma': [0.1, 0.25, 0.5, 0.6, 0.7, 0.75, 0.8]\n",
    "}\n",
    "\n",
    "default_params = {\n",
    "    'random_state':SEED\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "best_f1, best_recall, best_precision = outliers_grid(OneClassSVM, train, test, default_params, **params)\n",
    "print(best_f1)\n",
    "print()\n",
    "print(best_recall)\n",
    "print()\n",
    "print(best_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deskewed\n",
    "train = get_sample(deskewed, (50000,0))\n",
    "test = get_sample(deskewed[~deskewed.index.isin(train.index)], 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nu | gamma\n",
      "-----------------------------\n",
      "{'nu': 0.001, 'gamma': 0.01}\n",
      "{'nu': 0.001, 'gamma': 0.05}\n",
      "{'nu': 0.001, 'gamma': 0.1}\n",
      "{'nu': 0.001, 'gamma': 0.5}\n",
      "{'nu': 0.001, 'gamma': 0.6}\n",
      "{'nu': 0.001, 'gamma': 0.7}\n",
      "{'nu': 0.001, 'gamma': 0.75}\n",
      "{'nu': 0.001, 'gamma': 0.8}\n",
      "{'nu': 0.0025, 'gamma': 0.01}\n",
      "{'nu': 0.0025, 'gamma': 0.05}\n",
      "{'nu': 0.0025, 'gamma': 0.1}\n",
      "{'nu': 0.0025, 'gamma': 0.5}\n",
      "{'nu': 0.0025, 'gamma': 0.6}\n",
      "{'nu': 0.0025, 'gamma': 0.7}\n",
      "{'nu': 0.0025, 'gamma': 0.75}\n",
      "{'nu': 0.0025, 'gamma': 0.8}\n",
      "{'nu': 0.005, 'gamma': 0.01}\n",
      "{'nu': 0.005, 'gamma': 0.05}\n",
      "{'nu': 0.005, 'gamma': 0.1}\n",
      "{'nu': 0.005, 'gamma': 0.5}\n",
      "{'nu': 0.005, 'gamma': 0.6}\n",
      "{'nu': 0.005, 'gamma': 0.7}\n",
      "{'nu': 0.005, 'gamma': 0.75}\n",
      "{'nu': 0.005, 'gamma': 0.8}\n",
      "{'nu': 0.0075, 'gamma': 0.01}\n",
      "{'nu': 0.0075, 'gamma': 0.05}\n",
      "{'nu': 0.0075, 'gamma': 0.1}\n",
      "{'nu': 0.0075, 'gamma': 0.5}\n",
      "{'nu': 0.0075, 'gamma': 0.6}\n",
      "{'nu': 0.0075, 'gamma': 0.7}\n",
      "{'nu': 0.0075, 'gamma': 0.75}\n",
      "{'nu': 0.0075, 'gamma': 0.8}\n",
      "=========== DONE ==========\n",
      "{'nu': 0.001, 'gamma': 0.8, 'f1': 0.49193548387096775, 'recall': 0.5596330275229358, 'precision': 0.43884892086330934}\n",
      "\n",
      "{'nu': 0.005, 'gamma': 0.8, 'f1': 0.3392504930966469, 'recall': 0.7889908256880734, 'precision': 0.21608040201005024}\n",
      "\n",
      "{'nu': 0.001, 'gamma': 0.6, 'f1': 0.489795918367347, 'recall': 0.5504587155963303, 'precision': 0.4411764705882353}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'nu': [ 0.001, 0.0025, 0.005, 0.0075],\n",
    "    'gamma': [0.01, 0.05, 0.1, 0.5, 0.6, 0.7, 0.75, 0.8]\n",
    "}\n",
    "\n",
    "default_params = {\n",
    "    'random_state':SEED\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "best_f1, best_recall, best_precision = outliers_grid(OneClassSVM, train, test, default_params, **params)\n",
    "print(best_f1)\n",
    "print()\n",
    "print(best_recall)\n",
    "print()\n",
    "print(best_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like using scaled_df works the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run Through Whole DS\n",
    "train = get_sample(scaled_df, (50000,0))\n",
    "test = scaled_df\n",
    "\n",
    "xtrain = train[sub_cols]\n",
    "xtest = test[sub_cols]\n",
    "ytest = test.Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45319335083114604\n",
      "0.5264227642276422\n",
      "0.3978494623655914\n"
     ]
    }
   ],
   "source": [
    "# {'nu': 0.001, 'gamma': 0.75, 'f1': 0.5116279069767442, 'recall': 0.6055045871559633, 'precision': 0.4429530201342282}\n",
    "best_params = {'nu': 0.001, 'gamma': 0.75}\n",
    "\n",
    "ocsvm = OneClassSVM(**best_params)\n",
    "ocsvm.fit(xtrain)\n",
    "preds = ocsvm.predict(xtest)\n",
    "ypred = np.where(preds <0, 1, 0)\n",
    "print(metrics.f1_score(ytest, ypred))\n",
    "print(metrics.recall_score(ytest, ypred))\n",
    "print(metrics.precision_score(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outlier_ftr_df['ocsvm'] = ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_clusters(ytrue, labels, agg='sum'):\n",
    "    df = pd.DataFrame({'Class':ytrue, 'cluster_label':labels})\n",
    "    vcounts = df.groupby('cluster_label').Class.agg(['sum','size'])\n",
    "    rel_probs = vcounts['sum']*(vcounts['sum']/vcounts['size'])\n",
    "    score = rel_probs.sum() if agg=='sum' else rel_probs.mean() # mean penalizes larger number of clusters\n",
    "    return vcounts, score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def km_grid(Ks, xtrain, xtest, ytest, agg_func='sum', verbose=False):\n",
    "    best = {'k': 0, 'score':0, 'vcounts':None}\n",
    "    for k in Ks:\n",
    "        print('K=',k)\n",
    "        km = KMeans(n_clusters=k, n_init=5, n_jobs=-1, random_state=SEED)\n",
    "        km.fit(xtrain)\n",
    "        labels = km.predict(xtest)\n",
    "        vcounts, score = eval_clusters(ytest, labels, agg=agg_func)\n",
    "        if verbose:\n",
    "            print('score=',score)\n",
    "            print(vcounts)\n",
    "            print()\n",
    "        if score > best['score']:\n",
    "            best['k'] = k\n",
    "            best['score'] = score\n",
    "            best['vcounts'] = vcounts\n",
    "        \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 15\n",
      "K= 25\n",
      "K= 50\n",
      "K= 75\n",
      "K= 99\n",
      "K= 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'k': 99, 'score': 298.48608499090767, 'vcounts':                sum  size\n",
       " cluster_label           \n",
       " 0                2  2877\n",
       " 1                0  4194\n",
       " 2                2  3166\n",
       " 3                2  2915\n",
       " 4                4  3896\n",
       " 5                2  2638\n",
       " 6                0  3506\n",
       " 7                3  2676\n",
       " 8                0  3320\n",
       " 9                2   558\n",
       " 10               0  2976\n",
       " 11               0  3545\n",
       " 12               1  2044\n",
       " 13               0  1353\n",
       " 14               0  3204\n",
       " 15               2  2506\n",
       " 16               2  3758\n",
       " 17              38  2572\n",
       " 18               2  4310\n",
       " 19               0  1877\n",
       " 20               0  3887\n",
       " 21             237   270\n",
       " 22               0  2583\n",
       " 23               1  2567\n",
       " 24               1   849\n",
       " 25               0  3163\n",
       " 26               7  1651\n",
       " 27               0  2233\n",
       " 28               0  3125\n",
       " 29               0  2751\n",
       " ...            ...   ...\n",
       " 69               0  3589\n",
       " 70               0   532\n",
       " 71               0  4425\n",
       " 72               3  2460\n",
       " 73               1  2595\n",
       " 74               0  2867\n",
       " 75               0  2616\n",
       " 76               0    22\n",
       " 77               4  3065\n",
       " 78               0  3505\n",
       " 79               0  3521\n",
       " 80               1  2323\n",
       " 81               1  2187\n",
       " 82               0  3342\n",
       " 83               0  3915\n",
       " 84               1  3518\n",
       " 85               2  4689\n",
       " 86               1  5162\n",
       " 87               0  2531\n",
       " 88               2  3488\n",
       " 89               3  2719\n",
       " 90               0  4778\n",
       " 91               0   852\n",
       " 92               3  3248\n",
       " 93               0  4964\n",
       " 94               2  1638\n",
       " 95               7  1791\n",
       " 96               0  2534\n",
       " 97               0  2592\n",
       " 98               0  2549\n",
       " \n",
       " [99 rows x 2 columns]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# small training set\n",
    "train = get_sample(scaled_df, 15000, seed = SEED)\n",
    "test = scaled_df.copy()\n",
    "\n",
    "xtrain = train[sub_cols]\n",
    "xtest = test[sub_cols]\n",
    "ytest = test.Class\n",
    "\n",
    "best = km_grid([15,25,50,75,99,100], xtrain,xtest,ytest)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 10\n",
      "K= 15\n",
      "K= 25\n",
      "K= 50\n",
      "K= 75\n",
      "K= 99\n",
      "K= 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'k': 99, 'score': 326.75882809772486, 'vcounts':                sum  size\n",
       " cluster_label           \n",
       " 0                2  1557\n",
       " 1                0  4692\n",
       " 2                0  2953\n",
       " 3                1  2224\n",
       " 4                1  3075\n",
       " 5                0  4102\n",
       " 6                1  2788\n",
       " 7                3  3900\n",
       " 8                1  2244\n",
       " 9                1  4960\n",
       " 10               1  2519\n",
       " 11               2  3953\n",
       " 12               1  6121\n",
       " 13               0  2232\n",
       " 14               0  2536\n",
       " 15               0  1828\n",
       " 16               1  6981\n",
       " 17               2  3484\n",
       " 18               0  3203\n",
       " 19               9  2769\n",
       " 20               0  6078\n",
       " 21             107   134\n",
       " 22               2  3692\n",
       " 23               0  3610\n",
       " 24               1  3591\n",
       " 25               1  3437\n",
       " 26              29  2439\n",
       " 27               0  3270\n",
       " 28               6  1101\n",
       " 29               4  3211\n",
       " ...            ...   ...\n",
       " 69               0  3018\n",
       " 70               0   898\n",
       " 71               0  3493\n",
       " 72               0  3915\n",
       " 73               0  2205\n",
       " 74               0  1775\n",
       " 75               1   967\n",
       " 76               0  2067\n",
       " 77               1  2966\n",
       " 78               1  2370\n",
       " 79               1  3309\n",
       " 80               1  2845\n",
       " 81               0  2543\n",
       " 82               0  3324\n",
       " 83               3  3203\n",
       " 84               0  1628\n",
       " 85               0  3586\n",
       " 86               2  1473\n",
       " 87               0  4118\n",
       " 88               2  5132\n",
       " 89               2  3873\n",
       " 90               0  2274\n",
       " 91               0  2690\n",
       " 92               2   558\n",
       " 93               3  2148\n",
       " 94               2  3487\n",
       " 95               0  3177\n",
       " 96               0  1842\n",
       " 97               1  1493\n",
       " 98               0  2846\n",
       " \n",
       " [99 rows x 2 columns]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# larger training set\n",
    "train = get_sample(scaled_df, 50000, seed = SEED)\n",
    "test = scaled_df.copy()\n",
    "\n",
    "xtrain = train[sub_cols]\n",
    "xtest = test[sub_cols]\n",
    "ytest = test.Class\n",
    "\n",
    "best = km_grid([10,15,25,50, 75, 99, 100], xtrain,xtest,ytest)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=best['k'], n_init=5, n_jobs=-1, random_state=SEED)\n",
    "km.fit(xtrain)\n",
    "labels = km.predict(xtest)\n",
    "outlier_ftr_df['km_labels'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Based Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = get_sample(scaled_df, 15000, seed = SEED)\n",
    "test = scaled_df.copy()\n",
    "\n",
    "xtrain = train[sub_cols]\n",
    "xtest = test[sub_cols]\n",
    "ytest = test.Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gmm_grid(Ks, xtrain, xtest, ytest, verbose=False):\n",
    "    best = {'k': 0, 'score':0, 'probs':None}\n",
    "    for k in Ks:\n",
    "        print('K=',k)\n",
    "        mix = BayesGMM(n_components=k, n_init=5, covariance_type='full', random_state=SEED)\n",
    "        mix.fit(xtrain)\n",
    "        labels = mix.predict(xtest)\n",
    "        vcounts, score = eval_clusters(ytest, labels)\n",
    "        if verbose:\n",
    "            print('score=',score)\n",
    "            print(vcounts)\n",
    "            print()\n",
    "        if score > best['score']:\n",
    "            best['k'] = k\n",
    "            best['score'] = score\n",
    "            best['vcounts'] = vcounts\n",
    "        \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K= 5\n",
      "K= 10\n",
      "K= 15\n",
      "K= 25\n",
      "K= 50\n"
     ]
    }
   ],
   "source": [
    "best = gmm_grid([5,10,15,25,50], xtrain,xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k': 15,\n",
       " 'score': 326.91138561750154,\n",
       " 'probs': None,\n",
       " 'vcounts':                sum   size\n",
       " cluster_label            \n",
       " 0                2   2271\n",
       " 1                9   8739\n",
       " 2                4  34418\n",
       " 3                6   5747\n",
       " 4                5  34534\n",
       " 5                2   6934\n",
       " 6                5  23881\n",
       " 7                0  13377\n",
       " 8              394    475\n",
       " 9               14  12802\n",
       " 10              37  23122\n",
       " 11               2  23857\n",
       " 12               5  76511\n",
       " 13               6  10262\n",
       " 14               1   7877}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to feature df\n",
    "mix = BayesGMM(n_components=best['k'], n_init=5, covariance_type='full', random_state = SEED)\n",
    "mix.fit(xtrain)\n",
    "labels = mix.predict(xtest)\n",
    "outlier_ftr_df['gmm_labels'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write New Features To CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outlier_ftr_df.to_csv('Outlier_Ftrs.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
